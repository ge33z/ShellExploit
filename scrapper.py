import requests
from bs4 import BeautifulSoup
import re

def clean_url(url):
    cleaned_url = re.sub(r'/&ved=\S+', '', url)
    return cleaned_url

def is_google_url(url):
    return 'google.com' in url

def find_and_save():
    try:
        end_domain = input("Enter domain: ")
        with open('web.txt', 'a') as web_file:
            search_url = f"https://www.google.com/search?q=site%3A{end_domain}"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
            }

            try:
                response = requests.get(search_url, headers=headers)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, 'html.parser')
                links = soup.find_all('a', href=True)

                for link in links:
                    url = link['href']
                    match = re.search(r'(https?://\S+)', url)
                    if match:
                        found_url = match.group(1)
                        if end_domain in found_url and not is_google_url(found_url):
                            cleaned_url = clean_url(found_url)
                            print(f'Found: {cleaned_url}')
                            web_file.write(f'{cleaned_url}\n')

            except requests.exceptions.RequestException as e:
                print(f"Error accessing {search_url}: {e}")

    except Exception as e:
        print(f"An unexpected error occurred: {e}")

find_and_save()
